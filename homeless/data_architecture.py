#!/usr/bin/env python3
"""
Oregon Housing Analysis - Data Architecture
===========================================

This module implements a production-ready data architecture for housing analysis
including proper data models, naming conventions, and collection strategies.

Key Improvements:
1. Clear, accurate naming conventions
2. Comprehensive data model with proper relationships
3. Enhanced data sources and collection strategies
4. Data quality framework and validation
5. Production-ready error handling and monitoring
"""

import pandas as pd
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import json

class DataSource(Enum):
    """Enumeration of data sources for tracking data lineage"""
    CENSUS_DECENNIAL = "census_decennial"
    CENSUS_ACS = "census_acs"
    HUD_PIT = "hud_pit"
    BUILDING_PERMITS = "building_permits"
    INCOME_DATA = "income_data"
    LOCAL_SURVEYS = "local_surveys"
    SHELTER_DATA = "shelter_data"

class DataQualityLevel(Enum):
    """Data quality assessment levels"""
    EXCELLENT = "excellent"      # 95%+ confidence, multiple sources
    GOOD = "good"               # 90%+ confidence, reliable source
    FAIR = "fair"               # 80%+ confidence, some limitations
    POOR = "poor"               # <80% confidence, significant issues
    UNKNOWN = "unknown"         # Quality cannot be determined

@dataclass
class DataQualityMetrics:
    """Data quality assessment metrics"""
    completeness: float          # Percentage of expected data present
    accuracy: float             # Estimated accuracy based on source
    timeliness: float           # How current the data is
    consistency: float          # Internal consistency checks
    overall_score: DataQualityLevel
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "completeness": self.completeness,
            "accuracy": self.accuracy,
            "timeliness": self.timeliness,
            "consistency": self.consistency,
            "overall_score": self.overall_score.value
        }

class OregonHousingDataModel:
    """
    Professional data model for Oregon housing analysis
    
    This class defines the proper structure and relationships for:
    - Population data (total population counts)
    - Housing supply data (units, construction, permits)
    - Housing demand data (households, income, affordability)
    - Homeless data (actual counts, types, shelter capacity)
    - Housing gap analysis (supply, affordability, quality gaps)
    """
    
    def __init__(self):
        self.counties = self._get_oregon_counties()
        self.data_quality_framework = DataQualityFramework()
        
    def _get_oregon_counties(self) -> Dict[str, str]:
        """Get Oregon counties with FIPS codes and names"""
        return {
            "001": "Baker County",      # Smallest county by population
            "003": "Benton County",     # Home to Oregon State University
            "005": "Clackamas County",  # Suburban Portland area
            "007": "Clatsop County",    # Pacific coast county
            "009": "Columbia County",   # Northwest of Portland
            "011": "Coos County",       # South coast county
            "013": "Crook County",      # Central Oregon
            "015": "Curry County",      # Southwest coast
            "017": "Deschutes County",  # Bend area, fastest growing
            "019": "Douglas County",    # Roseburg area
            "021": "Gilliam County",    # Smallest county by area
            "023": "Grant County",      # Eastern Oregon
            "025": "Harney County",     # Largest county by area
            "027": "Hood River County", # Columbia Gorge
            "029": "Jackson County",    # Medford area
            "031": "Jefferson County",  # Central Oregon
            "033": "Josephine County",  # Grants Pass area
            "035": "Klamath County",    # Klamath Falls area
            "037": "Lake County",       # Eastern Oregon
            "039": "Lane County",       # Eugene area
            "041": "Lincoln County",    # Newport area
            "043": "Linn County",       # Albany area
            "045": "Malheur County",    # Eastern Oregon
            "047": "Marion County",     # Salem area (state capital)
            "049": "Morrow County",     # Eastern Oregon
            "051": "Multnomah County",  # Portland area (largest population)
            "053": "Polk County",       # West of Salem
            "055": "Sherman County",    # Eastern Oregon
            "057": "Tillamook County",  # Pacific coast
            "059": "Umatilla County",   # Eastern Oregon
            "061": "Union County",      # Eastern Oregon
            "063": "Wallowa County",    # Northeast Oregon
            "065": "Wasco County",      # The Dalles area
            "067": "Washington County", # Beaverton/Hillsboro area
            "069": "Wheeler County",    # Eastern Oregon
            "071": "Yamhill County"     # McMinnville area
        }
    
    def get_population_data_schema(self) -> Dict[str, Any]:
        """
        Schema for population data collection
        
        Returns:
            Dictionary defining the structure and validation rules
        """
        return {
            "table_name": "population_facts",
            "description": "Total population counts by county and year",
            "columns": {
                "year": {"type": "int", "constraints": ["not_null", "range:1990-2023"]},
                "county_fips": {"type": "str", "constraints": ["not_null", "length:3"]},
                "county_name": {"type": "str", "constraints": ["not_null"]},
                "total_population": {"type": "int", "constraints": ["not_null", "positive"]},
                "data_source": {"type": "str", "constraints": ["not_null", "enum:census_decennial,census_acs"]},
                "margin_of_error": {"type": "float", "constraints": ["nullable"]},
                "data_quality_score": {"type": "str", "constraints": ["not_null"]},
                "collection_date": {"type": "datetime", "constraints": ["not_null"]},
                "last_updated": {"type": "datetime", "constraints": ["not_null"]}
            },
            "primary_key": ["year", "county_fips"],
            "indexes": ["county_fips", "year", "data_source"]
        }
    
    def get_housing_supply_schema(self) -> Dict[str, Any]:
        """
        Schema for housing supply data collection
        
        Returns:
            Dictionary defining the structure and validation rules
        """
        return {
            "table_name": "housing_supply_facts",
            "description": "Housing supply metrics including units, construction, and permits",
            "columns": {
                "year": {"type": "int", "constraints": ["not_null", "range:2010-2023"]},
                "county_fips": {"type": "str", "constraints": ["not_null", "length:3"]},
                "county_name": {"type": "str", "constraints": ["not_null"]},
                "total_housing_units": {"type": "int", "constraints": ["not_null", "positive"]},
                "occupied_housing_units": {"type": "int", "constraints": ["not_null", "positive"]},
                "vacant_housing_units": {"type": "int", "constraints": ["not_null", "non_negative"]},
                "vacant_for_rent": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "vacant_for_sale": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "vacant_seasonal": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "vacant_other": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "building_permits_issued": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "new_construction_units": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "data_source": {"type": "str", "constraints": ["not_null", "enum:census_acs,building_permits"]},
                "data_quality_score": {"type": "str", "constraints": ["not_null"]},
                "collection_date": {"type": "datetime", "constraints": ["not_null"]},
                "last_updated": {"type": "datetime", "constraints": ["not_null"]}
            },
            "primary_key": ["year", "county_fips"],
            "indexes": ["county_fips", "year", "data_source"]
        }
    
    def get_housing_demand_schema(self) -> Dict[str, Any]:
        """
        Schema for housing demand data collection
        
        Returns:
            Dictionary defining the structure and validation rules
        """
        return {
            "table_name": "housing_demand_facts",
            "description": "Housing demand metrics including households, income, and affordability",
            "columns": {
                "year": {"type": "int", "constraints": ["not_null", "range:2010-2023"]},
                "county_fips": {"type": "str", "constraints": ["not_null", "length:3"]},
                "county_name": {"type": "str", "constraints": ["not_null"]},
                "total_households": {"type": "int", "constraints": ["not_null", "positive"]},
                "owner_occupied_households": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "renter_occupied_households": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "median_household_income": {"type": "int", "constraints": ["nullable", "positive"]},
                "median_gross_rent": {"type": "int", "constraints": ["nullable", "positive"]},
                "median_home_value": {"type": "int", "constraints": ["nullable", "positive"]},
                "affordability_index": {"type": "float", "constraints": ["nullable", "range:0-100"]},
                "cost_burdened_households": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "data_source": {"type": "str", "constraints": ["not_null", "enum:census_acs,income_data"]},
                "data_quality_score": {"type": "str", "constraints": ["not_null"]},
                "collection_date": {"type": "datetime", "constraints": ["not_null"]},
                "last_updated": {"type": "datetime", "constraints": ["not_null"]}
            },
            "primary_key": ["year", "county_fips"],
            "indexes": ["county_fips", "year", "data_source"]
        }
    
    def get_homeless_data_schema(self) -> Dict[str, Any]:
        """
        Schema for homeless data collection
        
        Returns:
            Dictionary defining the structure and validation rules
        """
        return {
            "table_name": "homeless_facts",
            "description": "Actual homeless counts and shelter capacity by county and year",
            "columns": {
                "year": {"type": "int", "constraints": ["not_null", "range:2010-2023"]},
                "county_fips": {"type": "str", "constraints": ["not_null", "length:3"]},
                "county_name": {"type": "str", "constraints": ["not_null"]},
                "total_homeless_count": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "sheltered_homeless": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "unsheltered_homeless": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "homeless_families": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "homeless_veterans": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "homeless_youth": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "shelter_capacity": {"type": "int", "constraints": ["nullable", "non_negative"]},
                "shelter_utilization_rate": {"type": "float", "constraints": ["nullable", "range:0-100"]},
                "data_source": {"type": "str", "constraints": ["not_null", "enum:hud_pit,local_surveys,shelter_data"]},
                "data_quality_score": {"type": "str", "constraints": ["not_null"]},
                "collection_date": {"type": "datetime", "constraints": ["not_null"]},
                "last_updated": {"type": "datetime", "constraints": ["not_null"]}
            },
            "primary_key": ["year", "county_fips"],
            "indexes": ["county_fips", "year", "data_source"]
        }
    
    def get_housing_gap_analysis_schema(self) -> Dict[str, Any]:
        """
        Schema for housing gap analysis
        
        Returns:
            Dictionary defining the structure and validation rules
        """
        return {
            "table_name": "housing_gap_analysis",
            "description": "Comprehensive housing gap analysis combining all data sources",
            "columns": {
                "year": {"type": "int", "constraints": ["not_null", "range:2010-2023"]},
                "county_fips": {"type": "str", "constraints": ["not_null", "length:3"]},
                "county_name": {"type": "str", "constraints": ["not_null"]},
                "total_population": {"type": "int", "constraints": ["not_null", "positive"]},
                "total_housing_units": {"type": "int", "constraints": ["not_null", "positive"]},
                "total_households": {"type": "int", "constraints": ["nullable", "positive"]},
                "supply_gap": {"type": "int", "constraints": ["nullable"]},  # housing_units_needed - housing_units_available
                "affordability_gap": {"type": "int", "constraints": ["nullable", "non_negative"]},  # households_unable_to_afford
                "homeless_gap": {"type": "int", "constraints": ["nullable", "non_negative"]},  # actual_homeless_count
                "housing_vacancy_rate": {"type": "float", "constraints": ["nullable", "range:0-100"]},
                "homeownership_rate": {"type": "float", "constraints": ["nullable", "range:0-100"]},
                "affordability_index": {"type": "float", "constraints": ["nullable", "range:0-100"]},
                "data_quality_score": {"type": "str", "constraints": ["not_null"]},
                "analysis_date": {"type": "datetime", "constraints": ["not_null"]},
                "last_updated": {"type": "datetime", "constraints": ["not_null"]}
            },
            "primary_key": ["year", "county_fips"],
            "indexes": ["county_fips", "year"],
            "foreign_keys": {
                "population_facts": ["year", "county_fips"],
                "housing_supply_facts": ["year", "county_fips"],
                "housing_demand_facts": ["year", "county_fips"],
                "homeless_facts": ["year", "county_fips"]
            }
        }

class DataQualityFramework:
    """
    Professional data quality assessment framework
    
    This class provides comprehensive data quality evaluation including:
    - Completeness checks
    - Accuracy assessments
    - Timeliness evaluation
    - Consistency validation
    - Overall quality scoring
    """
    
    def __init__(self):
        self.quality_thresholds = {
            "excellent": 0.95,
            "good": 0.90,
            "fair": 0.80,
            "poor": 0.70
        }
    
    def assess_completeness(self, df: pd.DataFrame, expected_counties: int = 36, expected_years: int = 14) -> float:
        """
        Assess data completeness based on expected coverage
        
        Args:
            df: DataFrame to assess
            expected_counties: Expected number of counties (36 for Oregon)
            expected_years: Expected number of years
            
        Returns:
            Completeness score as a percentage
        """
        if df.empty:
            return 0.0
        
        actual_records = len(df)
        expected_records = expected_counties * expected_years
        
        # Check for missing counties
        unique_counties = df['county_fips'].nunique()
        county_completeness = unique_counties / expected_counties
        
        # Check for missing years
        unique_years = df['year'].nunique()
        year_completeness = unique_years / expected_years
        
        # Overall completeness
        completeness = (county_completeness + year_completeness) / 2
        
        return completeness
    
    def assess_accuracy(self, data_source: str, year: int) -> float:
        """
        Assess data accuracy based on source and year
        
        Args:
            data_source: Source of the data
            year: Year of the data
            
        Returns:
            Accuracy score as a percentage
        """
        # Base accuracy scores by source
        source_accuracy = {
            DataSource.CENSUS_DECENNIAL.value: 0.99,    # Most accurate
            DataSource.CENSUS_ACS.value: 0.90,          # Good estimates
            DataSource.HUD_PIT.value: 0.85,             # Point-in-time counts
            DataSource.BUILDING_PERMITS.value: 0.95,    # Administrative data
            DataSource.INCOME_DATA.value: 0.88,         # Survey estimates
            DataSource.LOCAL_SURVEYS.value: 0.80,       # Varies by quality
            DataSource.SHELTER_DATA.value: 0.85         # Administrative data
        }
        
        base_accuracy = source_accuracy.get(data_source, 0.70)
        
        # Adjust for recency (newer data generally more accurate)
        current_year = datetime.now().year
        recency_factor = max(0.95, 1.0 - (current_year - year) * 0.01)
        
        return base_accuracy * recency_factor
    
    def assess_timeliness(self, collection_date: datetime, data_year: int) -> float:
        """
        Assess data timeliness
        
        Args:
            collection_date: When data was collected
            data_year: Year the data represents
            
        Returns:
            Timeliness score as a percentage
        """
        current_year = datetime.now().year
        current_month = datetime.now().month
        
        # Data should be available within 18 months of the year end
        expected_delay = 18
        actual_delay = (current_year - data_year) * 12 + (current_month - 12)
        
        if actual_delay <= expected_delay:
            return 1.0
        elif actual_delay <= expected_delay * 2:
            return 0.8
        elif actual_delay <= expected_delay * 3:
            return 0.6
        else:
            return 0.4
    
    def assess_consistency(self, df: pd.DataFrame) -> float:
        """
        Assess internal data consistency
        
        Args:
            df: DataFrame to assess
            
        Returns:
            Consistency score as a percentage
        """
        if df.empty:
            return 0.0
        
        consistency_checks = []
        
        # Check for logical consistency in housing data
        if 'total_housing_units' in df.columns and 'occupied_housing_units' in df.columns:
            if 'vacant_housing_units' in df.columns:
                # total = occupied + vacant
                logical_check = df['total_housing_units'] >= (df['occupied_housing_units'] + df['vacant_housing_units'])
                consistency_checks.append(logical_check.mean())
        
        # Check for reasonable value ranges
        if 'total_population' in df.columns:
            population_check = (df['total_population'] > 0) & (df['total_population'] < 10000000)
            consistency_checks.append(population_check.mean())
        
        # Check for missing values consistency
        missing_check = 1.0 - (df.isnull().sum().sum() / (len(df) * len(df.columns)))
        consistency_checks.append(missing_check)
        
        return sum(consistency_checks) / len(consistency_checks) if consistency_checks else 0.0
    
    def calculate_overall_quality(self, completeness: float, accuracy: float, 
                                timeliness: float, consistency: float) -> DataQualityLevel:
        """
        Calculate overall data quality level
        
        Args:
            completeness: Completeness score (0-1)
            accuracy: Accuracy score (0-1)
            timeliness: Timeliness score (0-1)
            consistency: Consistency score (0-1)
            
        Returns:
            Overall quality level
        """
        # Weighted average (accuracy and completeness most important)
        overall_score = (completeness * 0.25 + accuracy * 0.35 + 
                        timeliness * 0.20 + consistency * 0.20)
        
        if overall_score >= self.quality_thresholds["excellent"]:
            return DataQualityLevel.EXCELLENT
        elif overall_score >= self.quality_thresholds["good"]:
            return DataQualityLevel.GOOD
        elif overall_score >= self.quality_thresholds["fair"]:
            return DataQualityLevel.FAIR
        elif overall_score >= self.quality_thresholds["poor"]:
            return DataQualityLevel.POOR
        else:
            return DataQualityLevel.UNKNOWN
    
    def assess_dataset_quality(self, df: pd.DataFrame, data_source: str, 
                             collection_date: datetime, data_year: int) -> DataQualityMetrics:
        """
        Comprehensive data quality assessment
        
        Args:
            df: DataFrame to assess
            data_source: Source of the data
            collection_date: When data was collected
            data_year: Year the data represents
            
        Returns:
            DataQualityMetrics object with all quality scores
        """
        completeness = self.assess_completeness(df)
        accuracy = self.assess_accuracy(data_source, data_year)
        timeliness = self.assess_timeliness(collection_date, data_year)
        consistency = self.assess_consistency(df)
        
        overall_score = self.calculate_overall_quality(completeness, accuracy, timeliness, consistency)
        
        return DataQualityMetrics(
            completeness=completeness,
            accuracy=accuracy,
            timeliness=timeliness,
            consistency=consistency,
            overall_score=overall_score
        )

def main():
    """Demonstrate the professional data architecture"""
    print("üèóÔ∏è Oregon Housing Analysis - Professional Data Architecture")
    print("=" * 60)
    
    # Initialize the data model
    data_model = OregonHousingDataModel()
    
    # Display schemas
    print("\nüìä Data Schemas:")
    print("-" * 30)
    
    schemas = [
        ("Population Facts", data_model.get_population_data_schema()),
        ("Housing Supply Facts", data_model.get_housing_supply_schema()),
        ("Housing Demand Facts", data_model.get_housing_demand_schema()),
        ("Homeless Facts", data_model.get_homeless_data_schema()),
        ("Housing Gap Analysis", data_model.get_housing_gap_analysis_schema())
    ]
    
    for name, schema in schemas:
        print(f"\n{name}:")
        print(f"  Table: {schema['table_name']}")
        print(f"  Description: {schema['description']}")
        print(f"  Columns: {len(schema['columns'])}")
        print(f"  Primary Key: {schema['primary_key']}")
    
    # Demonstrate data quality framework
    print("\nüîç Data Quality Framework:")
    print("-" * 30)
    
    quality_framework = DataQualityFramework()
    
    # Example quality assessment
    print("\nExample Quality Assessment:")
    print("Source: Census ACS")
    print("Year: 2023")
    print("Collection Date: Now")
    
    # Create sample data for demonstration
    sample_df = pd.DataFrame({
        'year': [2023] * 36,
        'county_fips': [f"{i:03d}" for i in range(1, 37)],
        'total_population': [1000] * 36
    })
    
    quality_metrics = quality_framework.assess_dataset_quality(
        sample_df, 
        DataSource.CENSUS_ACS.value, 
        datetime.now(), 
        2023
    )
    
    print(f"\nQuality Scores:")
    print(f"  Completeness: {quality_metrics.completeness:.1%}")
    print(f"  Accuracy: {quality_metrics.accuracy:.1%}")
    print(f"  Timeliness: {quality_metrics.timeliness:.1%}")
    print(f"  Consistency: {quality_metrics.consistency:.1%}")
    print(f"  Overall: {quality_metrics.overall_score.value.title()}")
    
    print("\n‚úÖ Professional data architecture ready for implementation!")

if __name__ == "__main__":
    main()
